# Q-Learning 网格世界实现

## 项目简介

这是一个基于Python的Q-Learning强化学习算法实现，在5x5网格世界环境中训练智能体学习最优路径策略。智能体需要从起始位置(0,0)出发，避开陷阱，到达目标位置(4,4)。

## 环境要求

### 系统要求
- Python 3.6+
- NumPy库

### 安装依赖
```bash
pip install numpy
```

## 核心组件

### 1. GridWorld 环境类

**功能**: 定义5x5网格世界环境

**关键属性**:
- `size`: 网格大小 (默认5x5)
- `goal`: 目标位置 (4,4)
- `traps`: 陷阱位置 [(1,1), (2,3), (3,1)]
- `agent_pos`: 智能体当前位置
- `actions`: 四个动作方向 [上, 右, 下, 左]

**主要方法**:
- `reset()`: 重置环境到初始状态
- `step(action)`: 执行动作，返回新状态、奖励和是否结束
- `render_text()`: 文本形式显示当前环境状态

### 2. Q-Learning 算法

**核心参数**:
- `episodes`: 训练回合数 
- `alpha`: 学习率 
- `gamma`: 折扣因子 
- `epsilon`: 探索率 

## 算法原理

### Q-Learning 基础

Q-Learning是一种无模型的强化学习算法，通过学习状态-动作价值函数Q(s,a)来找到最优策略。

**Q值更新公式**:
```
Q(s,a) = Q(s,a) + α[r + γ·max(Q(s',a')) - Q(s,a)]
```

其中:
- `s`: 当前状态
- `a`: 当前动作
- `s'`: 下一状态
- `r`: 即时奖励
- `α`: 学习率
- `γ`: 折扣因子

### ε-贪婪策略

智能体使用ε-贪婪策略平衡探索与利用:
- 以概率ε随机选择动作（探索）
- 以概率(1-ε)选择Q值最大的动作（利用）

### 奖励机制

- **到达目标**: +10奖励，回合结束
- **掉入陷阱**: -10奖励，回合结束
- **普通移动**: -1奖励，鼓励寻找最短路径

## 使用方法

### 运行程序
```bash
python q_learning.py
```

### 输出说明

1. **训练过程**: 显示每50个回合的总奖励和Q表示例值
2. **训练统计**: 平均奖励、最大奖励、最小奖励
3. **学习策略**: 以箭头形式显示每个位置的最优动作

### 环境可视化

程序使用字符表示不同元素:
- `A`: 智能体位置
- `G`: 目标位置
- `T`: 陷阱位置
- `空格`: 普通位置

策略可视化使用方向箭头:
- `↑`: 向上移动
- `→`: 向右移动
- `↓`: 向下移动
- `←`: 向左移动

## 参数调优建议

### 学习率 (alpha)
- **较高值 (0.3-0.5)**: 快速学习，但可能不稳定
- **中等值 (0.1-0.2)**: 平衡学习速度和稳定性
- **较低值 (0.01-0.05)**: 稳定但学习缓慢

### 折扣因子 (gamma)
- **接近1 (0.9-0.99)**: 重视长期奖励
- **中等值 (0.7-0.8)**: 平衡短期和长期奖励
- **较低值 (0.5-0.6)**: 重视即时奖励

### 探索率 (epsilon)
- **训练初期**: 较高值 (0.3-0.5) 鼓励探索
- **训练后期**: 较低值 (0.05-0.1) 利用已学知识
- **可考虑**: 动态衰减策略






